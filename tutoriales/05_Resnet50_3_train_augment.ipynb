{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FUENTES**:\n",
    "\n",
    "PetFinder Kaggle:\n",
    "\n",
    "https://www.kaggle.com/competitions/petfinder-adoption-prediction/data\n",
    "\n",
    "First Tutorial:\n",
    "\n",
    "https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5\n",
    "\n",
    "Second Deep Tutorial:\n",
    "\n",
    "https://rumn.medium.com/part-1-ultimate-guide-to-fine-tuning-in-pytorch-pre-trained-model-and-its-configuration-8990194b71e\n",
    "\n",
    "Logo Recognition API:\n",
    "\n",
    "https://heartbeat.comet.ml/logo-recognition-ios-application-using-machine-learning-and-flask-api-aec4eff3be11\n",
    "\n",
    "Hybrid (multimodal) neural network architecture : Combination of tabular, textual and image inputs:\n",
    "\n",
    "https://medium.com/@dave.cote.msc/hybrid-multimodal-neural-network-architecture-combination-of-tabular-textual-and-image-inputs-7460a4f82a2e\n",
    "\n",
    "\n",
    "\n",
    "### **INDICACIONES PREVIAS**:\n",
    "\n",
    "+ **Git**:\n",
    "    + Clonamos el repo: root de todos los repos y ponemos git clone \"url_repo\"\n",
    "    + Hacemos el checkout de la rama main: git checkout -b new-branch\n",
    "\n",
    "+ **Poetry**:\n",
    "    + Instalamos poetry: https://python-poetry.org/docs/\n",
    "    + Realizamos un Update del pyproject: poetry update\n",
    "    + Activamos el entorno que creo poetry: poetry shell\n",
    "    + Intentamos correr una celda, si nos pide seleccionar el environment y no lo vemos en la lista, cerrar y volver abrir VSC\n",
    "\n",
    "+ **Torch y CUDA**:\n",
    "    + Verificar que versión pide torch:\n",
    "        + Versión de torch instalada: poetry show (en mi caso la 1.13.1)\n",
    "        + Buscar la versión correspondiente en la documentación: https://pytorch.org/get-started/previous-versions/  (en mi caso el 11.7)\n",
    "    + Instalar CUDA para Torch (buscar la versión correspondiente de CUDA): https://developer.nvidia.com/cuda-11-7-0-download-archive\n",
    "    + Verificar que CUDA esté funcional: correr en una celda torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import cv2\n",
    "#from PIL import Image\n",
    "#from pathlib import Path\n",
    "\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from joblib import load, dump\n",
    "\n",
    "from utils import plot_confusion_matrix\n",
    "\n",
    "# Appendeo el directorio de una carpeta arriba\n",
    "nb_dir = os.path.dirname(os.path.abspath(\"05_Resnet50_3_train_augment.ipynb\"))\n",
    "dir = os.path.abspath(os.path.join(nb_dir,'..'))\n",
    "sys.path.append(dir)\n",
    "\n",
    "from augment.autoaugment import ImageNetPolicy\t\n",
    "from augment.cutout import Cutout\n",
    "# Verificamos que CUDA está funcional\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seteo el Modelo**\n",
    "\n",
    "Teoría de Resnet: https://towardsdatascience.com/introduction-to-resnets-c0a830a288a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo modelo ResNet entrenado en Imagenet\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "# Modificar la última capa para adaptarse a tu problema específico\n",
    "num_ftrs = resnet50.fc.in_features\n",
    "resnet50.fc = torch.nn.Linear(num_ftrs, 5) # Clasificación 5 clases\n",
    "# Configuro para usar cuda si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet50 = resnet50.to(device)\n",
    "# Instancio del criterio de pérdida CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seteo parámetros, directorios y funciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_DIR = '../'\n",
    "PATH_TO_TRAIN = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train/train.csv\")\n",
    "PATH_TO_IMAGES_DIR = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train_images\")\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"work/optuna_temp_artifacts\")\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"work/optuna_artifacts\")\n",
    "\n",
    "MODEL_NAME = '04 ResNet Augment'\n",
    "\n",
    "MODEL_VERSION = '1.0.0'\n",
    "\n",
    "# Parametros y variables\n",
    "CREATE_PYTORCH_DIRECTORIES = 1\n",
    "SEED = 42\n",
    "BATCH_SIZE = 50\n",
    "TEST_SIZE = 0.2\n",
    "IMAGE_SIZE = 299\n",
    "CPU_CORES = os.cpu_count()\n",
    "\n",
    "# Armo el nuevo directorio de train\n",
    "new_train_directory = os.path.join(BASE_DIR, 'work/train_images_classes')\n",
    "os.makedirs(new_train_directory, exist_ok=True) # si ya existe el nombre, lo deja como está\n",
    "\n",
    "# Armo el nuevo directorio de validación\n",
    "new_val_directory = os.path.join(BASE_DIR, 'work/val_images_classes')\n",
    "os.makedirs(new_val_directory, exist_ok=True)\n",
    "\n",
    "# Definir las clases ordenadas\n",
    "class_names = ['0', '1', '2', '3', '4']\n",
    "\n",
    "# Mapear las etiquetas de las clases a números enteros consecutivos\n",
    "class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Creo las carpetas de clases dentro de los directorios\n",
    "for clase in class_names: # Una para cada clase\n",
    "   os.makedirs(os.path.join(new_train_directory, str(clase)), exist_ok=True)\n",
    "   os.makedirs(os.path.join(new_val_directory, str(clase)), exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Funciones para la carga y el preproceso\n",
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    # Calcula el factor de escala necesario para redimensionar la imagen de manera que el lado más largo tenga el tamaño deseado \n",
    "    ratio = float(IMAGE_SIZE)/max(old_size)\n",
    "    # Calcula las nuevas dimensiones de la imagen \n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # Redimensiona la imagen con el nuevo tamaño\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    # Calcula las diferencias de tamaño y agrega pixeles (color negro) en los extremos para que quede centrada y cuadrada \n",
    "    delta_w = IMAGE_SIZE - new_size[1]\n",
    "    delta_h = IMAGE_SIZE - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_image = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def load_image(pet_id):\n",
    "    path_to_image = os.path.join(PATH_TO_IMAGES_DIR, f'{pet_id}-1.jpg') # Irá a la primera imagen de la mascota\n",
    "    image = cv2.imread(path_to_image)\n",
    "    # Convierte la imagen de BGR a RGB porque estos modelos esperan ese orden de canales\n",
    "    image = cv2.convertScaleAbs(image)\n",
    "    image= cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    new_image = resize_to_square(image)\n",
    "    return new_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_pet(pet_id):\n",
    "    path_to_image = os.path.join(PATH_TO_IMAGES_DIR, f'{pet_id}-1.jpg') # Irá a la primera imagen de la mascota\n",
    "    # Cargar la imagen\n",
    "    image_to_show = cv2.imread(path_to_image)\n",
    "    # Convertir a formato RGB\n",
    "    image_to_show = cv2.cvtColor(image_to_show, cv2.COLOR_BGR2RGB)\n",
    "    # Visualizar la imagen\n",
    "    plt.imshow(image_to_show)\n",
    "    plt.axis('off')  # No mostrar los ejes\n",
    "    plt.show()\n",
    "\n",
    "def visualize_image(image):\n",
    "    # Convierte la imagen a un formato de enteros (CV_8U)\n",
    "    image = cv2.convertScaleAbs(image)\n",
    "    image= cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Visualizar la imagen\n",
    "    plt.imshow(image.astype(np.uint8))\n",
    "    plt.axis('off')  # No mostrar los ejes\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cargo y Proceso Data**\n",
    "\n",
    "Nota: Pytorch necesita que estén las imágenes en los distintos directorios según su clase y su participación en el training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completada la copia a:  ../work/train_images_classes\n",
      "Completada la copia a:  ../work/val_images_classes\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "# Cargo\n",
    "train_df = pd.read_csv(PATH_TO_TRAIN)\n",
    "\n",
    "# Split para validación\n",
    "train_data, val_data = train_test_split(train_df,\n",
    "                               test_size = TEST_SIZE,\n",
    "                               random_state = SEED,\n",
    "                               stratify = train_df.AdoptionSpeed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if CREATE_PYTORCH_DIRECTORIES == 1: # Poner en 0 si ya tengo las carpetas train_images_classes y val_images_classes con las imágenes copiadas\n",
    "    # Función para copiar las imágenes a los directorios correspondientes\n",
    "    def copy_imag(data, directorio_destino):\n",
    "        for index, row in data.iterrows():\n",
    "            petID = row['PetID']\n",
    "            adoption_speed = row['AdoptionSpeed']\n",
    "            \n",
    "            # Nombre del archivo de imagen\n",
    "            nombre_archivo = f\"{petID}-1.jpg\"\n",
    "            \n",
    "            # Ruta completa de la imagen de origen\n",
    "            ruta_origen = os.path.join(PATH_TO_IMAGES_DIR, nombre_archivo)\n",
    "            \n",
    "            # Ruta completa del directorio de destino\n",
    "            ruta_destino = os.path.join(directorio_destino, str(adoption_speed), nombre_archivo)\n",
    "            \n",
    "            # Verificar si el archivo de origen existe\n",
    "            if os.path.exists(ruta_origen):\n",
    "                # Copiar el archivo de origen al directorio de destino\n",
    "                shutil.copy2(ruta_origen, ruta_destino)\n",
    "        print(\"Completada la copia a: \",str(directorio_destino))\n",
    "\n",
    "    # Copiar las imágenes al directorio de train\n",
    "    copy_imag(train_data, new_train_directory)\n",
    "\n",
    "    # Copiar las imágenes al directorio de val\n",
    "    copy_imag(val_data, new_val_directory)\n",
    "\n",
    "    print(\"Proceso completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero los DataLoaders\n",
    "def create_dataloaders(train_directory, val_directory, batch_size, num_workers):\n",
    "    # Transformaciones de imagen para el conjunto de entrenamiento\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        ImageNetPolicy(),  ############### LAS POLÍTICAS AUTOAUGMENT PARA IMAGENET (AUGMENT)\n",
    "        transforms.ToTensor(),\n",
    "        Cutout(n_holes=1, length = 16),   ############### CUTOUT PARA REGULARIZACIÓN (AUGMENT)\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Transformaciones de imagen para el conjunto de validación (sin data augment)\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Crear conjuntos de datos para el conjunto de entrenamiento y validación\n",
    "    conjunto_entrenamiento = datasets.ImageFolder(train_directory, transform=train_transforms)\n",
    "    conjunto_validacion = datasets.ImageFolder(val_directory, transform=val_transforms)\n",
    "\n",
    "    # Asignar las clases ordenadas al conjunto de datos\n",
    "    conjunto_entrenamiento.class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "    conjunto_validacion.class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "    # Crear dataloaders para el conjunto de entrenamiento y validación\n",
    "    train_dataloader = torch.utils.data.DataLoader(conjunto_entrenamiento, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_dataloader = torch.utils.data.DataLoader(conjunto_validacion, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "# Aplico las funcion de los DataLoaders\n",
    "train_dataloader, val_dataloader = create_dataloaders(new_train_directory , new_val_directory , BATCH_SIZE, CPU_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero una lista de PetIDs con imagen en el orden en que aparecen en el data loader\n",
    "test_sample_ids = [i[0].split('/')[-1].split('-')[0] for i in val_dataloader.dataset.samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entreno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:38<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4247 Acc: 30.72% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3890 Acc: 33.21% Kappa: 0.265\n",
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3788 Acc: 34.40% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3753 Acc: 33.81% Kappa: 0.288\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3613 Acc: 35.88% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3677 Acc: 34.78% Kappa: 0.297\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3451 Acc: 37.33% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3608 Acc: 35.31% Kappa: 0.299\n",
      "Training complete in 7m 14s\n",
      "Best val Acc: 35.31%\n",
      "Modelo guardado en ../work/optuna_temp_artifacts/04 ResNet Augment_1.0.0_20250410_182633.pth\n"
     ]
    }
   ],
   "source": [
    "def train_val(model, criterion, dataloaders, datasets, device, num_epochs=20, lr=0.001, momentum = 0.9 ,trial=None):\n",
    "    \n",
    "    # Instancio Stochastic Gradient Descent (SGD): Defino el parámetro del Learning Rate (define \"el paso\" en que avanzan los pesos en cada iteración) y el Momentum (pone innercia a la dirección del gradiente descendiente para que no cambie de dirección en minimos locales)\n",
    "    optimizer = optim.SGD(resnet50.parameters(), lr=lr, momentum=momentum) # Parámetros default del SGD\n",
    "    \n",
    "    #Inicializo variables\n",
    "    since = time.time()\n",
    "\n",
    "    #Inicializo variable de mejor kappa entre trials\n",
    "    try:\n",
    "        #Intento obtener el mejor kappa de optuna\n",
    "        previous_best = study.best_value\n",
    "    except:\n",
    "        #Si no hay, seteo -999\n",
    "        previous_best = -999\n",
    "\n",
    "    #Inicializo variables de mejor modelo y mejor accuracy y mejor kappa de este trial\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_kappa =  -999\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        #Inicializo listas de kappa true y predicted y scores para esta epoch\n",
    "        epoch_kappa_labels_true = []\n",
    "        epoch_kappa_labels_predicted = []\n",
    "        epoch_output_scores = []\n",
    "\n",
    "        #Cada epoch tiene una fase de entrenamiento y validación\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            #Inicializo variables de loss y accuracy para esta fase de epoch\n",
    "            epoch_phase_running_loss = 0.0\n",
    "            epoch_phase_running_corrects = 0\n",
    "\n",
    "            # Itero sobre los datos.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    elif phase == 'val':\n",
    "                        #Agrego los valores de kappa true y predicted para cada batch en validación\n",
    "                        epoch_kappa_labels_true.extend(labels.cpu().numpy().tolist())\n",
    "                        epoch_kappa_labels_predicted.extend(preds.cpu().numpy().tolist())\n",
    "                        outputs_np = outputs.cpu().numpy()\n",
    "                        epoch_output_scores.extend([outputs_np[i,:] for i in range(outputs_np.shape[0])])\n",
    "\n",
    "                # Statistics for each phase\n",
    "                epoch_phase_running_loss += loss.item() * inputs.size(0)\n",
    "                epoch_phase_running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                #END OF BATCH\n",
    "\n",
    "            epoch_loss = epoch_phase_running_loss / len(datasets[phase])\n",
    "            epoch_acc = epoch_phase_running_corrects.double() / len(datasets[phase])\n",
    "            \n",
    "            #Calculo el kappa para cada epoch\n",
    "            if phase == 'train':\n",
    "                #overall_train_losses.append(epoch_loss)\n",
    "                current_kappa_score = np.nan\n",
    "            else:\n",
    "                #overall_val_losses.append(epoch_loss)\n",
    "                current_kappa_score = cohen_kappa_score(epoch_kappa_labels_true,\n",
    "                                  epoch_kappa_labels_predicted,\n",
    "                                  weights = 'quadratic')\n",
    "                    \n",
    "\n",
    "\n",
    "            print(f'{phase.title()} Loss: {epoch_loss:.4f} Acc: {epoch_acc*100:.2f}% Kappa: {current_kappa_score:.3f}')\n",
    "\n",
    "            # If this is the best Epoch so far -> Deep copy the model\n",
    "            if phase == 'val' and current_kappa_score > best_kappa:\n",
    "                best_acc = epoch_acc\n",
    "                best_kappa = current_kappa_score\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "                #Best Epoch within a trial and better than previous trials\n",
    "                if trial is not None and best_kappa > previous_best:\n",
    "\n",
    "                    #Save test dataset with predictions\n",
    "                    predicted_filename = os.path.join(PATH_TO_TEMP_FILES,f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "                    predicted_df = pd.DataFrame({'PetID':test_sample_ids,\n",
    "                                'pred':epoch_output_scores}).merge(val_data, on='PetID')\n",
    "                    dump(predicted_df, predicted_filename)\n",
    "\n",
    "                    #Generate and save CM \n",
    "                    cm_filename = os.path.join(PATH_TO_TEMP_FILES,f'cm_{trial.study.study_name}_{trial.number}.jpg')\n",
    "                    plot_confusion_matrix(epoch_kappa_labels_true,epoch_kappa_labels_predicted).write_image(cm_filename)\n",
    "\n",
    "            #END OF PHASE\n",
    "\n",
    "        #END OF EPOCH\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.2f}%'.format(best_acc * 100))\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Save in optuna trial the best test dataset, cm and model weights\n",
    "    if trial is not None and best_kappa > previous_best:\n",
    "        upload_artifact(trial, predicted_filename, artifact_store)   \n",
    "\n",
    "        upload_artifact(trial, cm_filename, artifact_store)\n",
    "\n",
    "        file_name = f'{MODEL_NAME}_{MODEL_VERSION}_{trial.number}.pth'\n",
    "        model_path = os.path.join(PATH_TO_TEMP_FILES, file_name)\n",
    "        torch.save(model, model_path) # Podemos guardar solo los pesos si queremos: best_model.state_dict()\n",
    "        upload_artifact(trial, model_path, artifact_store)\n",
    "\n",
    "    return model,best_kappa\n",
    "\n",
    "best_model,_ = train_val(resnet50, criterion, \n",
    "                       dataloaders={'train': train_dataloader, \n",
    "                                    'val': val_dataloader}, \n",
    "                       datasets={'train': train_data, 'val': val_data}, \n",
    "                       device=device, \n",
    "                       num_epochs=4)\n",
    "# Guardo el modelo\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_name = f'{MODEL_NAME}_{MODEL_VERSION}_{run_id}.pth'\n",
    "model_path = os.path.join(PATH_TO_TEMP_FILES, file_name)\n",
    "torch.save(best_model, model_path) # Podemos guardar solo los pesos si queremos: best_model.state_dict()\n",
    "print(f'Modelo guardado en {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75963/1318953185.py:1: ExperimentalWarning: FileSystemArtifactStore is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "  artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n"
     ]
    }
   ],
   "source": [
    "artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "\n",
    "def optuna_train(trial):\n",
    "\n",
    "    epochs = trial.suggest_int('epochs', 5, 5)\n",
    "\n",
    "    lr = trial.suggest_float('lr', 0.00001, 0.1, log=True)\n",
    "\n",
    "    momentum = trial.suggest_float('momentum', 0.0, 0.95)\n",
    "\n",
    "    _,best_score = train_val(resnet50, criterion, \n",
    "                       dataloaders={'train': train_dataloader, \n",
    "                                    'val': val_dataloader}, \n",
    "                       datasets={'train': train_data, 'val': val_data}, \n",
    "                       device=device, \n",
    "                       num_epochs=epochs,\n",
    "                       lr=lr,\n",
    "                       momentum = momentum,\n",
    "                       trial=trial)\n",
    "\n",
    "\n",
    "    return(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-10 18:26:34,329] A new study created in RDB with name: 04 ResNet Augment_1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3312 Acc: 38.00% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3608 Acc: 35.31% Kappa: 0.299\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3322 Acc: 37.98% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3617 Acc: 35.28% Kappa: 0.297\n",
      "Epoch 2/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3308 Acc: 37.75% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3607 Acc: 35.38% Kappa: 0.302\n",
      "Epoch 3/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3306 Acc: 37.84% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3617 Acc: 35.01% Kappa: 0.301\n",
      "Epoch 4/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3328 Acc: 37.77% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.43it/s]\n",
      "/tmp/ipykernel_75963/1627552036.py:126: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n",
      "/tmp/ipykernel_75963/1627552036.py:128: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n",
      "/tmp/ipykernel_75963/1627552036.py:133: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3610 Acc: 35.15% Kappa: 0.302\n",
      "Training complete in 9m 3s\n",
      "Best val Acc: 35.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-10 18:35:37,582] Trial 0 finished with value: 0.3022931046856231 and parameters: {'epochs': 5, 'lr': 0.00012292882447111764, 'momentum': 0.4181374911166359}. Best is trial 0 with value: 0.3022931046856231.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3310 Acc: 37.63% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3615 Acc: 34.81% Kappa: 0.300\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3303 Acc: 37.87% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3614 Acc: 35.15% Kappa: 0.301\n",
      "Epoch 2/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3305 Acc: 37.92% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3601 Acc: 35.45% Kappa: 0.300\n",
      "Epoch 3/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3289 Acc: 38.15% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3610 Acc: 35.51% Kappa: 0.307\n",
      "Epoch 4/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3254 Acc: 38.45% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.43it/s]\n",
      "/tmp/ipykernel_75963/1627552036.py:126: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n",
      "/tmp/ipykernel_75963/1627552036.py:128: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n",
      "/tmp/ipykernel_75963/1627552036.py:133: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3599 Acc: 35.38% Kappa: 0.302\n",
      "Training complete in 9m 2s\n",
      "Best val Acc: 35.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-10 18:44:39,637] Trial 1 finished with value: 0.3071410395937758 and parameters: {'epochs': 5, 'lr': 0.0005701865660761107, 'momentum': 0.16119293526347694}. Best is trial 1 with value: 0.3071410395937758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3437 Acc: 36.86% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3595 Acc: 35.41% Kappa: 0.311\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3020 Acc: 40.20% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3776 Acc: 34.54% Kappa: 0.298\n",
      "Epoch 2/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:37<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2546 Acc: 43.30% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:10<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4323 Acc: 35.11% Kappa: 0.317\n",
      "Epoch 3/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 199/235 [01:14<00:13,  2.66it/s]\n",
      "[W 2025-04-10 18:51:20,584] Trial 2 failed with parameters: {'epochs': 5, 'lr': 0.0033601787153221007, 'momentum': 0.9385757383150473} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_75963/1318953185.py\", line 12, in optuna_train\n",
      "    _,best_score = train_val(resnet50, criterion,\n",
      "  File \"/tmp/ipykernel_75963/1627552036.py\", line 70, in train_val\n",
      "    epoch_phase_running_loss += loss.item() * inputs.size(0)\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-10 18:51:20,585] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                             storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///../work/db.sqlite3\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Specify the storage URL here.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m                             study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                             load_if_exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptuna_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36moptuna_train\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      8\u001b[0m lr \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.00001\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m momentum \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.95\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m _,best_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(best_score)\n",
      "Cell \u001b[0;32mIn[10], line 70\u001b[0m, in \u001b[0;36mtrain_val\u001b[0;34m(model, criterion, dataloaders, datasets, device, num_epochs, lr, momentum, trial)\u001b[0m\n\u001b[1;32m     67\u001b[0m         epoch_output_scores\u001b[38;5;241m.\u001b[39mextend([outputs_np[i,:] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(outputs_np\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])])\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Statistics for each phase\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m epoch_phase_running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     71\u001b[0m epoch_phase_running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#END OF BATCH\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize',\n",
    "                            storage=\"sqlite:///../work/db.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=f'{MODEL_NAME}_{MODEL_VERSION}',\n",
    "                            load_if_exists = True)\n",
    "study.optimize(optuna_train, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldi2_cuda_trf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
